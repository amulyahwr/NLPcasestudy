{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research4/projects/topic_modeling_autoencoding/fidgit\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 21725\n"
     ]
    }
   ],
   "source": [
    "#Loading vocab\n",
    "vocab ='./data/vocab.pkl'\n",
    "vocab = pickle.load(open(vocab,'rb'))\n",
    "vocab_size = len(vocab)\n",
    "swapped_vocab_docu = dict((v,k) for k,v in vocab.items())\n",
    "print(\"Vocab Size: %d\"%(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global 1: Topic Embeddings vs Word Embeddings (Un-Modified) vs Word Embeddings (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv.pop(1)\n",
    "sys.argv.pop(1)\n",
    "sys.path.insert(0, '/research4/projects/topic_modeling_autoencoding/fidgit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Topic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETM Topic Embeddings Shape: (90, 300)\n"
     ]
    }
   ],
   "source": [
    "#Load topic embeddings\n",
    "etm_topic_embs = pickle.load(open('./myetm/results/model_0_concept_embed.pickle','rb')).detach().cpu().numpy()\n",
    "print(\"ETM Topic Embeddings Shape: \"+str(etm_topic_embs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft VQ-VAE Topic Embeddings Shape: (90, 300)\n"
     ]
    }
   ],
   "source": [
    "soft_topic_embs = pickle.load(open('./vqvae_soft/results/model_0_concept_embed.pickle','rb')).cpu().numpy()\n",
    "print(\"Soft VQ-VAE Topic Embeddings Shape: \"+str(soft_topic_embs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Un-modified Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un-Modified Word Embeddings Shape: torch.Size([21725, 300])\n"
     ]
    }
   ],
   "source": [
    "unmodified_word_embs = os.path.join('./data/', 'embed.pth')\n",
    "unmodified_word_embs = torch.load(unmodified_word_embs).cuda()\n",
    "print(\"Un-Modified Word Embeddings Shape: \"+str(unmodified_word_embs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Modified Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard-vqvae model\n",
    "from vqvae_hard.model import TopicVAE as hard\n",
    "from vqvae_hard.config import parse_args as hard_parse_args\n",
    "\n",
    "hard_args = hard_parse_args()\n",
    "hard_args.numbr_concepts = 20\n",
    "hard_pretrained = hard(hard_args, vocab_20ng)\n",
    "hard_checkpoint = torch.load('./vqvae_hard/checkpoints/model_8.pt')\n",
    "hard_pretrained.load_state_dict(hard_checkpoint['model'])\n",
    "hard_pretrained.cuda()\n",
    "hard_pretrained.eval()\n",
    "_, hard_modified_word_embs , _, _ = hard_pretrained.vqvae(unmodified_word_embs, \"test\")\n",
    "hard_modified_word_embs = hard_modified_word_embs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soft-vqvae model\n",
    "from vqvae_soft.model import TopicVAE as soft\n",
    "from vqvae_soft.config import parse_args as soft_parse_args\n",
    "\n",
    "soft_args = soft_parse_args()\n",
    "soft_args.numbr_concepts = 20\n",
    "soft_pretrained = soft(soft_args, vocab_20ng)\n",
    "soft_checkpoint = torch.load('./vqvae_soft/checkpoints/model_8.pt')\n",
    "soft_pretrained.load_state_dict(soft_checkpoint['model'])\n",
    "soft_pretrained.cuda()\n",
    "soft_pretrained.eval()\n",
    "_, _, soft_modified_word_embs , _, _ = soft_pretrained.vqvae(unmodified_word_embs, \"test\")\n",
    "soft_modified_word_embs = soft_modified_word_embs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-vqvae model views\n",
    "from vqvae_multi.model import TopicVAE as multi\n",
    "from vqvae_multi.config import parse_args as multi_parse_args\n",
    "\n",
    "multi_args = multi_parse_args()\n",
    "multi_args.numbr_concepts = 20\n",
    "heads = 8\n",
    "multi_pretrained = multi(multi_args, vocab_20ng)\n",
    "multi_checkpoint = torch.load('./vqvae_multi/checkpoints/model_20.pt')\n",
    "multi_pretrained.load_state_dict(multi_checkpoint['model'])\n",
    "multi_pretrained.cuda()\n",
    "multi_pretrained.eval()\n",
    "_, multi_modified_word_embs , _, _ = multi_pretrained.vqvae(unmodified_word_embs, \"test\")\n",
    "multi_modified_word_embs = multi_modified_word_embs.cpu().numpy()\n",
    "\n",
    "multi_topic_embs = torch.from_numpy(multi_topic_embs).cuda().repeat(1, heads)\n",
    "if heads == 2:\n",
    "    multi_topic_embs = torch.tanh(multi_pretrained.ff_1(multi_topic_embs))\n",
    "elif heads == 4:\n",
    "    multi_topic_embs = torch.tanh(multi_pretrained.ff_2(multi_topic_embs))\n",
    "    multi_topic_embs = torch.tanh(multi_pretrained.ff_1(multi_topic_embs))\n",
    "elif heads == 8:\n",
    "    multi_topic_embs = torch.tanh(multi_pretrained.ff_3(multi_topic_embs))\n",
    "    multi_topic_embs = torch.tanh(multi_pretrained.ff_2(multi_topic_embs))\n",
    "    multi_topic_embs = torch.tanh(multi_pretrained.ff_1(multi_topic_embs))\n",
    "multi_topic_embs = multi_topic_embs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /research4/projects/topic_modeling_autoencoding/20ng/dvtm_bc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DVITM\n",
    "from model import DVTM as model_dvtm\n",
    "from config import parse_args as parse_args\n",
    "\n",
    "args = parse_args()\n",
    "args.numbr_concepts = 20\n",
    "dvtm_pretrained = model_dvtm(args, vocab_20ng)\n",
    "dvtm_checkpoint = torch.load('./msft_checkpoints/checkpoints_numbr_concepts20_latent_dim16_kld_wt5e-05_bcbf4096.0.pt')\n",
    "dvtm_pretrained.load_state_dict(dvtm_checkpoint['model'])\n",
    "dvtm_pretrained.eval()\n",
    "dvtm_pretrained = dvtm_pretrained.cuda()\n",
    "thetas, log_thetas, logits_thetas = dvtm_pretrained.encoder(unmodified_word_embs, \"test\")\n",
    "scores = torch.nn.functional.gumbel_softmax(logits_thetas, tau=dvtm_pretrained.temp, hard=False, eps=dvtm_pretrained.eps)\n",
    "soft_modified_word_embs = torch.matmul(scores, dvtm_pretrained.emb_concept.weight)\n",
    "soft_modified_word_embs = soft_modified_word_embs.view(-1, dvtm_pretrained.latent_dim * dvtm_pretrained.in_dim)\n",
    "soft_modified_word_embs = torch.relu(dvtm_pretrained.dec_0(soft_modified_word_embs))\n",
    "# soft_modified_word_embs = dvtm_pretrained.dec_1(soft_modified_word_embs)\n",
    "\n",
    "# soft_modified_word_embs = soft_modified_word_embs.mean(dim=1)\n",
    "\n",
    "soft_modified_word_embs = soft_modified_word_embs.detach().cpu().numpy()\n",
    "pca_dvtm = PCA(n_components=300)\n",
    "soft_modified_word_embs = pca_dvtm.fit_transform(soft_modified_word_embs)\n",
    "soft_modified_word_embs = soft_modified_word_embs/np.max(soft_modified_word_embs, axis=1, keepdims=True)\n",
    "# print(soft_modified_word_embs)\n",
    "soft_topic_embs = dvtm_pretrained.emb_concept.weight.detach().cpu().numpy()\n",
    "print(soft_modified_word_embs.shape)\n",
    "print(soft_topic_embs.shape)\n",
    "\n",
    "# args_with_lts = parse_args_with_lts()\n",
    "# args_with_lts.numbr_concepts = 20\n",
    "# dvtm_with_lts_pretrained = model_dvtm_with_lts(args_with_lts, vocab_20ng)\n",
    "# dvtm_with_lts_checkpoint = torch.load('./checkpoints/%s%d.pt'%('model_', expno))\n",
    "# dvtm_with_lts_pretrained.load_state_dict(dvtm_with_lts_checkpoint['model'])\n",
    "# dvtm_with_lts_pretrained.eval()\n",
    "# dvtm_with_lts_pretrained = dvtm_with_lts_pretrained.cuda()\n",
    "# thetas, log_thetas = dvtm_with_lts_pretrained.encoder(unmodified_word_embs, \"test\")\n",
    "# scores, _ = dvtm_with_lts_pretrained.rt(thetas, log_thetas, len(unmodified_word_embs), \"test\")\n",
    "# dvtm_with_lts_modified_word_embs = torch.matmul(scores, soft_topic_embs)\n",
    "# soft_modified_word_embs = dvtm_with_lts_modified_word_embs.detach().cpu().numpy()\n",
    "# soft_topic_embs = soft_topic_embs.cpu()\n",
    "# print(soft_modified_word_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmodified_word_embs = unmodified_word_embs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top moved words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by calculating distance between Modified and Unmodifed word embeddings\n",
    "numbr_top_words = 1000\n",
    "data = []\n",
    "for word, idx in vocab_20ng.items():\n",
    "#     if word in ['jet','mile','km','train','road','honda','bmw','motor','bus','rear','engine','wheel','brake']:\n",
    "    dist = np.linalg.norm(soft_modified_word_embs[idx]-unmodified_word_embs[idx], ord=2)\n",
    "    data.append((word, idx, dist))\n",
    "\n",
    "data = np.array(data)\n",
    "df = pd.DataFrame(data, columns=['word','idx','dist'])\n",
    "df = df.sort_values(by='dist', ascending=False)\n",
    "top_moved_words = df.head(numbr_top_words)\n",
    "print(top_moved_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_top_modified_word_embs = []\n",
    "soft_top_unmodified_word_embs = []\n",
    "etm_top_unmodified_word_embs = []\n",
    "\n",
    "considered_lst_words = []\n",
    "count = 0\n",
    "for wrd, idx in zip(top_moved_words['word'], top_moved_words['idx']):\n",
    "    soft_top_modified_word_embs.append(soft_modified_word_embs[int(idx)])\n",
    "    soft_top_unmodified_word_embs.append(unmodified_word_embs[int(idx)])\n",
    "    etm_top_unmodified_word_embs.append(unmodified_word_embs[int(idx)])\n",
    "    \n",
    "#     if wrd in ['jet','mile','km','train','road','honda','bmw','motor','bus','rear','engine','wheel','brake']: #soft\n",
    "#         considered_lst_words.append(count)\n",
    "#     if wrd in ['mit','cambridge','library','department','facility','professor','scientist','foundation','period','nasa','corporation']:\n",
    "#         considered_lst_words.append(count)\n",
    "#     if wrd in ['university', 'group', 'institute', 'information', 'computer', 'year', 'science', 'research', 'college', 'education', 'program', 'professor', 'school', 'engineering']: #hard\n",
    "#         considered_lst_words.append(count)\n",
    "#israel, people, war, israeli, jews, turkish, armenians, country, armenian, government\n",
    "#     if wrd in ['jew', 'hitler', 'surrender', 'massacre', 'mary', 'ignorance', 'atheism']: #multi\n",
    "#         considered_lst_words.append(count)\n",
    "    \n",
    "#     if wrd in ['mit', 'engineering', 'holy', 'ab', 'biblical', 'prophet', 'nec', 'bible', 'wiretap', 'simm']: #multi\n",
    "#     if wrd in ['palestinian','troops','security','foreign','government']:\n",
    "#     considered_lst_words.append(count)\n",
    "#     if wrd in ['unix', 'server', 'os', 'macintosh', 'dos', 'windows', 'ftp', 'mac', 'software', 'amiga'\n",
    "#               'use', 'drive', 'system', 'card', 'run', 'disk', 'problem', 'image']:\n",
    "#         considered_lst_words.append(count)\n",
    "\n",
    "#     if wrd in ['lebanese', 'resistance', 'israeli', 'buffer', 'zone', 'lebanon', 'israel', 'palestinian', 'civilian', 'bomb',\n",
    "#               'people', 'war', 'jews', 'turkish', 'armenians', 'country', 'armenian', 'government']: #dvitm without lts\n",
    "#         considered_lst_words.append(count)\n",
    "    if wrd in ['hockey', 'canada', 'state', 'cover', 'southern', 'board', 'engineering', 'canadian', 'helmet',\n",
    "              'team', 'division', 'san', 'nhl', 'toronto', 'york']: #dvitm with lts\n",
    "        considered_lst_words.append(count)\n",
    "    count = count + 1\n",
    "    \n",
    "soft_top_modified_word_embs = np.array(soft_top_modified_word_embs)\n",
    "soft_top_unmodified_word_embs = np.array(soft_top_unmodified_word_embs)\n",
    "etm_top_unmodified_word_embs = np.array(etm_top_unmodified_word_embs)\n",
    "\n",
    "print(\"Soft Top Modified Word embeddings shape: \"+ str(soft_top_modified_word_embs.shape))\n",
    "print(\"Soft Top Unmodified Word embeddings shape: \"+str(soft_top_unmodified_word_embs.shape))\n",
    "print(\"ETM Top Unmodified Word embeddings shape: \"+str(etm_top_unmodified_word_embs.shape))\n",
    "\n",
    "pca_data = np.concatenate((soft_topic_embs, etm_topic_embs, soft_top_modified_word_embs, soft_top_unmodified_word_embs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_result = pca.fit_transform(pca_data)\n",
    "\n",
    "x_pca  = pca_result[:,0]\n",
    "y_pca = pca_result[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_lst_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pca = make_subplots(rows=1, cols=1, \n",
    "                    shared_yaxes= True, \n",
    "                    shared_xaxes= True)\n",
    "\n",
    "num_topics = soft_topic_embs.shape[0]\n",
    "# soft_topic_trace = go.Scatter(x=x_pca[:num_topics],\n",
    "#                             y=y_pca[:num_topics],\n",
    "#                             mode='markers+text',\n",
    "#                             name='Topics',\n",
    "#                             legendgroup = 'a',\n",
    "#                             textposition=\"bottom center\",\n",
    "#                             text = np.arange(num_topics),\n",
    "#                             marker=dict(color='red'))\n",
    "\n",
    "soft_topic_trace = go.Scatter(x=[x_pca[13]],\n",
    "                            y=[y_pca[13]],\n",
    "                            mode='markers+text',\n",
    "                            name='Topics',\n",
    "                            legendgroup = 'a',\n",
    "                            textposition=\"bottom center\",\n",
    "                            text = ['13'],\n",
    "                            marker=dict(color='red'))\n",
    "\n",
    "\n",
    "shapes = []\n",
    "for idx in considered_lst_words:\n",
    "        \n",
    "    soft_mod_trace = go.Scatter(x=[x_pca[2*num_topics+idx]],\n",
    "                                y=[y_pca[2*num_topics+idx]],\n",
    "                                mode='markers',\n",
    "                                name='Modified Words',\n",
    "                                #textposition=\"bottom center\",\n",
    "                                text=list(top_moved_words['word'])[idx],\n",
    "                                legendgroup = 'b',\n",
    "                                marker=dict(color='green'))\n",
    "\n",
    "    unmod_trace = go.Scatter(x=[x_pca[2*num_topics+numbr_top_words+idx]],\n",
    "                                y=[y_pca[2*num_topics+numbr_top_words+idx]],\n",
    "                                mode='markers',\n",
    "                                name='Un-Modified Words',\n",
    "                                #textposition=\"bottom center\",\n",
    "                                text=list(top_moved_words['word'])[idx],\n",
    "                                legendgroup = 'c',\n",
    "                                marker=dict(color='lightgreen'))\n",
    "\n",
    "# soft_mod_trace = go.Scatter(x=x_pca[2*num_topics:2*num_topics+numbr_top_words],\n",
    "#                             y=y_pca[2*num_topics:2*num_topics+numbr_top_words],\n",
    "#                             mode='markers',\n",
    "#                             name='Modified Words',\n",
    "#                             #textposition=\"bottom center\",\n",
    "#                             text=list(top_moved_words['word']),\n",
    "#                             legendgroup = 'b',\n",
    "#                             marker=dict(color='green'))\n",
    "\n",
    "# unmod_trace = go.Scatter(x=x_pca[2*num_topics+numbr_top_words:],\n",
    "#                             y=y_pca[2*num_topics+numbr_top_words:],\n",
    "#                             mode='markers',\n",
    "#                             name='Un-Modified Words',\n",
    "#                             #textposition=\"bottom center\",\n",
    "#                             text=list(top_moved_words['word']),\n",
    "#                             legendgroup = 'c',\n",
    "#                             marker=dict(color='lightgreen'))\n",
    "\n",
    "    fig_pca.append_trace(unmod_trace,1,1)\n",
    "    fig_pca.append_trace(soft_mod_trace,1,1)\n",
    "\n",
    "\n",
    "\n",
    "    shapes.append(go.layout.Shape(\n",
    "                type=\"line\",\n",
    "            x0=x_pca[2*num_topics+idx],\n",
    "            y0=y_pca[2*num_topics+idx],\n",
    "            x1=x_pca[2*num_topics+numbr_top_words+idx],\n",
    "            y1=y_pca[2*num_topics+numbr_top_words+idx],\n",
    "            line=dict(\n",
    "                color=\"grey\",\n",
    "                width=0.5,\n",
    "                dash=\"dot\",\n",
    "            )\n",
    "        ))\n",
    "\n",
    "fig_pca.append_trace(soft_topic_trace,1,1)\n",
    "# shapes = []\n",
    "# for x1,x0,y1,y0 in zip(x_pca[2*num_topics:2*num_topics+numbr_top_words], \\\n",
    "#                     x_pca[2*num_topics+numbr_top_words:], \\\n",
    "#                     y_pca[2*num_topics:2*num_topics+numbr_top_words], \\\n",
    "#                     y_pca[2*num_topics+numbr_top_words:]):\n",
    "#     shapes.append(go.layout.Shape(\n",
    "#                 type=\"line\",\n",
    "#             x0=x0,\n",
    "#             y0=y0,\n",
    "#             x1=x1,\n",
    "#             y1=y1,\n",
    "#             line=dict(\n",
    "#                 color=\"grey\",\n",
    "#                 width=0.5,\n",
    "#                 dash=\"dot\",\n",
    "#             )\n",
    "#         ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etm_topic_trace = go.Scatter(x=x_pca[num_topics:2*num_topics],\n",
    "#                             y=y_pca[num_topics:2*num_topics],\n",
    "#                             mode='markers+text',\n",
    "#                             name='Topics',\n",
    "#                             legendgroup = 'a',\n",
    "#                             textposition=\"bottom center\",\n",
    "#                             text = np.arange(num_topics),\n",
    "#                             marker=dict(color='purple'))\n",
    "\n",
    "etm_topic_trace = go.Scatter(x=[x_pca[num_topics+6]],\n",
    "                            y=[y_pca[num_topics+6]],\n",
    "                            mode='markers+text',\n",
    "                            name='Topics',\n",
    "                            legendgroup = 'a',\n",
    "                            textposition=\"bottom center\",\n",
    "                            text = ['6'],\n",
    "                            marker=dict(color='purple'))\n",
    "\n",
    "#fig_pca.append_trace(unmod_trace,1,1)\n",
    "fig_pca.append_trace(etm_topic_trace,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pca.update_layout(shapes= shapes, showlegend=False, title=\"Topic Embeddings and Word Embeddings (Modified and Un-modified) for Soft VQ-VAE and ETM\")    \n",
    "fig_pca.update_layout(width=1000, height=600)\n",
    "fig_pca.show()\n",
    "#Show specific example of \n",
    "#1. word bmw, in Soft-VQ-VAE Topic = 9, In ETM Topic = 8\n",
    "#2. word univeristy, in Soft-VQ-VAE Topic = 15, In ETM Topic = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global2: Beta matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProdLDA\n",
    "expno = 8\n",
    "prodlda_beta = pickle.load(open('/research4/projects/topic_modeling_autoencoding/20ng/myprodlda/results/model_%d_concept_embed.pickle'%(expno),'rb')).cpu()\n",
    "prodlda_beta = torch.softmax(prodlda_beta, dim=1)\n",
    "print(\"ProdLDA Beta Shape: %s\"%(str(prodlda_beta.shape)))\n",
    "\n",
    "#ETM\n",
    "etm_beta = torch.matmul(torch.from_numpy(etm_topic_embs).cuda(), unmodified_word_embs.t())\n",
    "etm_beta = torch.softmax(etm_beta, dim=1)\n",
    "print(\"ETM Beta Shape: %s\"%(str(etm_beta.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /research4/projects/topic_modeling_autoencoding/20ng/dvtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DVITM without lts loss\n",
    "from model import DVTM as model_dvtm_without_lts\n",
    "from config import parse_args as parse_args_without_lts\n",
    "\n",
    "args_without_lts = parse_args_without_lts()\n",
    "args_without_lts.numbr_concepts = 20\n",
    "dvtm_without_lts_pretrained = model_dvtm_without_lts(args_without_lts, vocab_20ng)\n",
    "dvtm_without_lts_checkpoint = torch.load('./msft_checkpoints/checkpoints_numbr_concepts20_latent_dim16_kld_wt5e-05.pt')\n",
    "dvtm_without_lts_pretrained.load_state_dict(dvtm_without_lts_checkpoint['model'])\n",
    "dvtm_without_lts_pretrained = dvtm_without_lts_pretrained.cuda()\n",
    "dvtm_without_lts_pretrained.eval()\n",
    "tmp_scores = torch.zeros((args_without_lts.numbr_concepts, dvtm_without_lts_pretrained.latent_dim, args_without_lts.numbr_concepts)).cuda()\n",
    "for idx in range(args_without_lts.numbr_concepts):\n",
    "    tmp_scores[idx, :, idx] = 1\n",
    "\n",
    "out = torch.matmul(tmp_scores, dvtm_without_lts_pretrained.emb_concept.weight)\n",
    "out = out.view(-1, dvtm_without_lts_pretrained.latent_dim * dvtm_without_lts_pretrained.in_dim)\n",
    "out = torch.relu(dvtm_without_lts_pretrained.dec_0(out))\n",
    "out = dvtm_without_lts_pretrained.dec_1(out)\n",
    "dvtm_without_lts_beta = torch.softmax(out, dim=1)\n",
    "print(\"DVTM without lts beta Shape: \"+str(dvtm_without_lts_beta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /research4/projects/topic_modeling_autoencoding/20ng/dvtm_bc_loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DVITM with lts loss\n",
    "from model import DVTM as model_dvtm_with_lts\n",
    "from config import parse_args as parse_args_with_lts\n",
    "\n",
    "# expno = 12\n",
    "# soft_topic_embs = pickle.load(open('/research4/projects/topic_modeling_autoencoding/20ng/dvtm_bc_loss/results/model_%d_concept_embed.pickle'%(expno),'rb'))\n",
    "# expno = 30\n",
    "# soft_topic_embs = pickle.load(open('/research4/projects/topic_modeling_autoencoding/20ng/dvtm/results_prev/model_%d_concept_embed.pickle'%(expno),'rb'))\n",
    "args_with_lts = parse_args_with_lts()\n",
    "args_with_lts.numbr_concepts = 20\n",
    "dvtm_with_lts_pretrained = model_dvtm_with_lts(args_with_lts, vocab_20ng)\n",
    "dvtm_with_lts_checkpoint = torch.load('./msft_checkpoints/checkpoints_numbr_concepts20_latent_dim16_kld_wt5e-05_bcbf4096.0.pt')\n",
    "dvtm_with_lts_pretrained.load_state_dict(dvtm_with_lts_checkpoint['model'])\n",
    "dvtm_with_lts_pretrained = dvtm_with_lts_pretrained.cuda()\n",
    "dvtm_with_lts_pretrained.eval()\n",
    "\n",
    "tmp_scores = torch.zeros((args_with_lts.numbr_concepts, dvtm_with_lts_pretrained.latent_dim, args_with_lts.numbr_concepts)).cuda()\n",
    "for idx in range(args_with_lts.numbr_concepts):\n",
    "    tmp_scores[idx, :, idx] = 1\n",
    "\n",
    "out = torch.matmul(tmp_scores, dvtm_with_lts_pretrained.emb_concept.weight)\n",
    "out = out.view(-1, dvtm_with_lts_pretrained.latent_dim * dvtm_with_lts_pretrained.in_dim)\n",
    "out = torch.relu(dvtm_with_lts_pretrained.dec_0(out))\n",
    "out = dvtm_with_lts_pretrained.dec_1(out)\n",
    "dvtm_with_lts_beta = torch.softmax(out, dim=1)\n",
    "\n",
    "print(\"DVTM with lts beta Shape: \"+str(dvtm_with_lts_beta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soft-VQVAE\n",
    "soft_beta = torch.matmul(torch.from_numpy(soft_topic_embs), torch.from_numpy(unmodified_word_embs).t())\n",
    "soft_beta = torch.softmax(soft_beta, dim=1)\n",
    "\n",
    "#Hard-VQVAE\n",
    "hard_beta = torch.matmul(torch.from_numpy(hard_topic_embs), torch.from_numpy(unmodified_word_embs).t())\n",
    "hard_beta = torch.softmax(hard_beta, dim=1)\n",
    "\n",
    "#Multi-VQVAE\n",
    "multi_beta = torch.matmul(torch.from_numpy(multi_topic_embs), torch.from_numpy(unmodified_word_embs).t())\n",
    "multi_beta = torch.softmax(multi_beta, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /research4/projects/topic_modeling_autoencoding/20ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "lda_data = []\n",
    "with open('../mymallet/20ng/20ng_output/train_tww_20.txt','r') as lda_beta_file:\n",
    "    for line in lda_beta_file.readlines():\n",
    "        line_split = line.split('\\t')\n",
    "        topic = int(line_split[0])\n",
    "        word = line_split[1]\n",
    "        unnormalized_wt = float(line_split[2].replace('\\n',''))\n",
    "        lda_data.append((topic, word, unnormalized_wt))\n",
    "\n",
    "lda_df = pd.DataFrame(lda_data, columns=['topic','word','uwt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vocab = list(lda_df['word'])\n",
    "lda_vocab_lst = sorted(list(set(lda_vocab)))\n",
    "lda_vocab_idx = np.arange(len(lda_vocab))\n",
    "lda_vocab = {}\n",
    "for word, idx in zip(lda_vocab_lst, lda_vocab_idx):\n",
    "    lda_vocab[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_beta = []\n",
    "\n",
    "for topic_idx in np.unique(sorted(list(lda_df['topic']))):\n",
    "    topic_data = lda_df[lda_df['topic']==topic_idx]\n",
    "    topic_data = topic_data.sort_values(by='word')\n",
    "    lda_beta.append(np.array(list(topic_data['uwt'])))\n",
    "\n",
    "lda_words = np.unique(sorted(list(topic_data['word'])))\n",
    "lda_beta = np.array(lda_beta)\n",
    "lda_beta = lda_beta/np.sum(lda_beta, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(topk_indices, vocab):\n",
    "    arr_words = []\n",
    "    for idx in tqdm_notebook(topk_indices):\n",
    "        words = []\n",
    "        for word_idx in idx:\n",
    "            words.append(list(vocab.keys())[list(vocab.values()).index(word_idx)])\n",
    "        words =np.array(words)\n",
    "        arr_words.append(words)\n",
    "    \n",
    "    \n",
    "    return np.array(arr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topk_values, lda_topk_indices = torch.topk(torch.from_numpy(lda_beta).cuda(),10, dim=1)\n",
    "prodlda_topk_values, prodlda_topk_indices = torch.topk(prodlda_beta,10, dim=1)\n",
    "etm_topk_values, etm_topk_indices = torch.topk(etm_beta,10, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvtm_without_lts_topk_values, dvtm_without_lts_topk_indices = torch.topk(dvtm_without_lts_beta,10, dim=1)\n",
    "dvtm_with_lts_topk_values, dvtm_with_lts_topk_indices = torch.topk(dvtm_with_lts_beta,10, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_topk_values, soft_topk_indices = torch.topk(soft_beta,10, dim=1)\n",
    "hard_topk_values, hard_topk_indices = torch.topk(hard_beta,10, dim=1)\n",
    "multi_topk_values, multi_topk_indices = torch.topk(multi_beta,10, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_words = idx2word(lda_topk_indices, lda_vocab)\n",
    "prodlda_words = idx2word(prodlda_topk_indices, vocab_20ng)\n",
    "etm_words = idx2word(etm_topk_indices, vocab_20ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvtm_without_lts_words = idx2word(dvtm_without_lts_topk_indices, vocab_20ng)\n",
    "dvtm_with_lts_words = idx2word(dvtm_with_lts_topk_indices, vocab_20ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_words = idx2word(soft_topk_indices, vocab_20ng)\n",
    "hard_words = idx2word(hard_topk_indices, vocab_20ng)\n",
    "multi_words = idx2word(multi_topk_indices, vocab_20ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words = np.union1d(np.union1d(np.union1d(lda_words, np.union1d(etm_words, soft_words)),hard_words),multi_words)\n",
    "all_words = np.union1d(np.union1d(np.union1d(lda_words, np.union1d(etm_words, prodlda_words)),dvtm_without_lts_words),dvtm_with_lts_words)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sub_beta = []\n",
    "prodlda_sub_beta = []\n",
    "etm_sub_beta = []\n",
    "dvtm_without_lts_sub_beta = []\n",
    "dvtm_with_lts_sub_beta = []\n",
    "# soft_sub_beta = []\n",
    "# hard_sub_beta = []\n",
    "# multi_sub_beta = []\n",
    "\n",
    "\n",
    "final_word_list = []\n",
    "for word in all_words:\n",
    "    if word in lda_vocab.keys() and word in vocab_20ng.keys():\n",
    "        lda_sub_beta.append(torch.from_numpy(lda_beta).cuda()[:, lda_vocab[word]:lda_vocab[word]+1])\n",
    "        prodlda_sub_beta.append(prodlda_beta[:, vocab_20ng[word]:vocab_20ng[word]+1])\n",
    "        etm_sub_beta.append(etm_beta[:, vocab_20ng[word]:vocab_20ng[word]+1])\n",
    "        dvtm_without_lts_sub_beta.append(dvtm_without_lts_beta[:, vocab_20ng[word]:vocab_20ng[word]+1])\n",
    "        dvtm_with_lts_sub_beta.append(dvtm_with_lts_beta[:, vocab_20ng[word]:vocab_20ng[word]+1])\n",
    "#         soft_sub_beta.append(soft_beta[:, vocab_20ng[word]:vocab_20ng[word]+1])\n",
    "#         hard_sub_beta.append(hard_beta[:, vocab_20ng[word]:vocab_20ng[word]+1])\n",
    "#         multi_sub_beta.append(multi_beta[:, vocab_20ng[word]:vocab_20ng[word]+1])\n",
    "        final_word_list.append(word)\n",
    "        \n",
    "lda_sub_beta = torch.cat(lda_sub_beta, dim=1).cpu().numpy()\n",
    "prodlda_sub_beta = torch.cat(prodlda_sub_beta, dim=1).cpu().numpy()\n",
    "etm_sub_beta = torch.cat(etm_sub_beta, dim=1).cpu().numpy()\n",
    "dvtm_without_lts_sub_beta = torch.cat(dvtm_without_lts_sub_beta, dim=1).detach().cpu().numpy()\n",
    "dvtm_with_lts_sub_beta = torch.cat(dvtm_with_lts_sub_beta, dim=1).detach().cpu().numpy()\n",
    "\n",
    "# soft_sub_beta = torch.cat(soft_sub_beta,dim=1).cpu().numpy()\n",
    "# hard_sub_beta = torch.cat(hard_sub_beta,dim=1).cpu().numpy()\n",
    "# multi_sub_beta = torch.cat(multi_sub_beta,dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "fig_beta = make_subplots(rows=5, cols=1, \n",
    "                    shared_yaxes= True,\n",
    "                    subplot_titles=(\"CGS-LDA\", \"ProdLDA\",\"ETM\", \"DVITM w/o lts loss\", \"DVITM with lts loss\"))  \n",
    "\n",
    "lda_trace = go.Heatmap(z = lda_sub_beta,\n",
    "                      x = final_word_list,\n",
    "                      y=np.arange(num_topics),\n",
    "                      showscale=False,\n",
    "                      colorscale='algae')\n",
    "\n",
    "prodlda_trace = go.Heatmap(z = prodlda_sub_beta,\n",
    "                      x = final_word_list,\n",
    "                      y=np.arange(num_topics),\n",
    "                      showscale=False,\n",
    "                      colorscale='algae')\n",
    "\n",
    "etm_trace = go.Heatmap(z = etm_sub_beta,\n",
    "                      x = final_word_list,\n",
    "                      y=np.arange(num_topics),\n",
    "                      showscale=False,\n",
    "                      colorscale='algae')\n",
    "dvtm_without_lts_trace = go.Heatmap(z = dvtm_without_lts_sub_beta,\n",
    "                      x = final_word_list,\n",
    "                      y=np.arange(num_topics),\n",
    "                      showscale=False,\n",
    "                      colorscale='algae')\n",
    "\n",
    "dvtm_with_lts_trace = go.Heatmap(z = dvtm_with_lts_sub_beta,\n",
    "                      x = final_word_list,\n",
    "                      y=np.arange(num_topics),\n",
    "                      showscale=False,\n",
    "                      colorscale='algae')\n",
    "\n",
    "# hard_trace = go.Heatmap(z = hard_sub_beta,\n",
    "#                         x = final_word_list,\n",
    "#                         y=np.arange(num_topics),\n",
    "#                        showscale=False,\n",
    "#                       colorscale='algae')\n",
    "\n",
    "# soft_trace = go.Heatmap(z = soft_sub_beta,\n",
    "#                         x = final_word_list,\n",
    "#                         y=np.arange(num_topics),\n",
    "#                        showscale=False,\n",
    "#                       colorscale='algae')\n",
    "\n",
    "# multi_trace = go.Heatmap(z = multi_sub_beta,\n",
    "#                         x = final_word_list,\n",
    "#                         y=np.arange(num_topics),\n",
    "#                        showscale=False,\n",
    "#                       colorscale='algae')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_beta.append_trace(lda_trace, 1, 1)\n",
    "fig_beta.append_trace(prodlda_trace, 2, 1)\n",
    "fig_beta.append_trace(etm_trace, 3, 1)\n",
    "fig_beta.append_trace(dvtm_without_lts_trace, 4, 1)\n",
    "fig_beta.append_trace(dvtm_with_lts_trace, 5, 1)\n",
    "# fig_beta.append_trace(hard_trace, 3, 1)\n",
    "# fig_beta.append_trace(soft_trace, 4, 1)\n",
    "# fig_beta.append_trace(multi_trace, 5, 1)\n",
    "\n",
    "fig_beta.update_layout(width=1000, height=1500)\n",
    "fig_beta.update_yaxes(nticks=num_topics, autorange=\"reversed\",type='category')\n",
    "fig_beta.update_xaxes(tickangle=45,type='category')\n",
    "fig_beta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local1: Document specific topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /research4/projects/topic_modeling_autoencoding/20ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test dataset\n",
    "test_file = '/research4/projects/topic_modeling_autoencoding/20ng/data/test.pth'\n",
    "test_dataset = torch.load(test_file)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(newsgroups_test.data))\n",
    "for idx, doc in enumerate(newsgroups_test.data):\n",
    "#     print(doc)\n",
    "    doc_low = doc.lower()\n",
    "    words_doc_low = doc_low.split()\n",
    "#     if ('boston' in words_doc_low) and ('puck' in words_doc_low) and ('college' in words_doc_low) and ('baseball' in words_doc_low):\n",
    "    if ('david' in words_doc_low) and ('science' in words_doc_low) and ('department' in words_doc_low) and ('research' in words_doc_low):\n",
    "        print(idx)\n",
    "        if idx == 6937:\n",
    "            print(doc)\n",
    "        print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "lda_doc_topics_data = []\n",
    "with open('/research4/projects/topic_modeling_autoencoding/mymallet/20ng/20ng_output/test_doc_topics_20.txt','r') as lda_doc_topics_file:\n",
    "    for line in lda_doc_topics_file.readlines()[1:]:\n",
    "        line_split = line.split('\\t')\n",
    "        #print(line_split[1])\n",
    "        filename_split = line_split[1].split('/')\n",
    "\n",
    "        filename_idx = int(filename_split[len(filename_split)-1].replace('.txt',''))\n",
    "\n",
    "        topic_proportion = np.array(list(map(float, line_split[2:])))\n",
    "        lda_doc_topics_data.append((filename_idx, topic_proportion))\n",
    "\n",
    "lda_doc_topic_df = pd.DataFrame(lda_doc_topics_data, columns=['doc_idx','topic_proportion'])\n",
    "lda_doc_topic_df = lda_doc_topic_df.sort_values(by='doc_idx')\n",
    "lda_doc_topic_df = lda_doc_topic_df.reset_index(drop=True)\n",
    "lda_theta = np.array(list(lda_doc_topic_df['topic_proportion'])[:-1])\n",
    "print(\"LDA Test Dataset Topic Proportion Shape: \"+ str(lda_theta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prodlda model\n",
    "from myprodlda.model import ProdLDA as my_prodlda\n",
    "from myprodlda.config import parse_args as myprodlda_parse_args\n",
    "\n",
    "myprodlda_args = myprodlda_parse_args()\n",
    "myprodlda_args.numbr_concepts = 20\n",
    "myprodlda_pretrained = my_prodlda(myprodlda_args, vocab_20ng)\n",
    "myprodlda_checkpoint = torch.load('/research4/projects/topic_modeling_autoencoding/20ng/myprodlda/checkpoints/model_8.pt')\n",
    "myprodlda_pretrained.load_state_dict(myprodlda_checkpoint['model'])\n",
    "myprodlda_pretrained.cuda()\n",
    "myprodlda_pretrained.eval()\n",
    "_, _, _, myprodlda_theta = myprodlda_pretrained(test_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myprodlda_theta = myprodlda_theta.detach().cpu().numpy()\n",
    "print(\"ProdLDA Test Dataset Topic Proportion Shape: \"+ str(myprodlda_theta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etm model\n",
    "from myetm.model import ETM as my_etm\n",
    "from myetm.config import parse_args as myetm_parse_args\n",
    "\n",
    "myetm_args = myetm_parse_args()\n",
    "myetm_args.numbr_concepts = 20\n",
    "myetm_pretrained = my_etm(myetm_args, vocab_20ng)\n",
    "myetm_checkpoint = torch.load('/research4/projects/topic_modeling_autoencoding/20ng/myetm/checkpoints/myetm_0.pt')\n",
    "myetm_pretrained.load_state_dict(myetm_checkpoint['model'])\n",
    "myetm_pretrained.cuda()\n",
    "myetm_pretrained.eval()\n",
    "# _, myetm_normalized_bows, _ = myetm_pretrained.convert2bow(test_dataset, \"test\")\n",
    "myetm_reconst_loss, myetm_theta, _, _= myetm_pretrained(test_dataset, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myetm_theta = myetm_theta.detach().cpu().numpy()\n",
    "myetm_reconst_loss = myetm_reconst_loss.detach().cpu().numpy()\n",
    "print(\"ETM Test Dataset Topic Proportion Shape: \"+ str(myetm_theta.shape))\n",
    "print(\"ETM Test Dataset Reconstructtion Loss shape: \"+ str(myetm_reconst_loss.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppl from myetm\n",
    "# import math\n",
    "# etm_ppl = math.exp(myetm_reconst_loss[3844]/len(test_dataset[3844]))\n",
    "# lda_ppl = math.exp(141.4214211499424/len(test_dataset[3844]))\n",
    "# print(lda_ppl)\n",
    "# print(etm_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get actual tokens\n",
    "docu_tokens_lst = []\n",
    "for docu in test_dataset:\n",
    "    tokens_lst = []\n",
    "    for idx in docu.numpy():\n",
    "        tokens_lst.append(swapped_vocab_docu[idx])\n",
    "    tokens_str = ' '.join(tokens_lst)\n",
    "    docu_tokens_lst.append(tokens_str)\n",
    "print(len(docu_tokens_lst))\n",
    "print(docu_tokens_lst[3844])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dvtms\n",
    "import math\n",
    "\n",
    "dvtm_without_lts_theta_lst = []\n",
    "dvtm_with_lts_theta_lst = []\n",
    "ppl_data = []\n",
    "\n",
    "for idx, docu in enumerate(tqdm(test_dataset, desc='Please wait..')):\n",
    "    docu = docu.cuda()\n",
    "    myetm_ppl = math.exp(myetm_reconst_loss[idx]/len(docu))\n",
    "    dvtm_without_lts_outputs, _ ,dvtm_without_lts_thetas = dvtm_without_lts_pretrained(docu, \"test\")\n",
    "#     print(dvtm_without_lts_thetas.shape)\n",
    "    dvtm_without_lts_thetas = dvtm_without_lts_thetas.mean(dim=1)\n",
    "#     dvtm_without_lts_thetas, _ = torch.max(dvtm_without_lts_thetas, dim=1)\n",
    "    dvtm_without_lts_ppl = math.exp(-torch.sum(dvtm_without_lts_outputs)/len(docu))\n",
    "    dvtm_without_lts_theta = torch.sum(dvtm_without_lts_thetas, dim=0, keepdim=True)\n",
    "    dvtm_without_lts_theta = dvtm_without_lts_theta/torch.sum(dvtm_without_lts_theta)\n",
    "#     topk_val, topk_idxs = torch.topk(dvtm_without_lts_theta, k=3, dim=1)\n",
    "#     topk_idxs = torch.squeeze(topk_idxs)\n",
    "#     topk_idxs = topk_idxs.detach().cpu().numpy()\n",
    "#     topk_idxs_without_lts = set(topk_idxs.tolist())\n",
    "    \n",
    "    dvtm_with_lts_outputs, _ ,dvtm_with_lts_thetas = dvtm_with_lts_pretrained(docu, \"test\")\n",
    "    dvtm_with_lts_thetas = dvtm_with_lts_thetas.mean(dim=1)\n",
    "#     print(dvtm_with_lts_thetas.shape)\n",
    "#     dvtm_with_lts_thetas, _ = torch.max(dvtm_with_lts_thetas, dim=1)\n",
    "    dvtm_with_lts_ppl = math.exp(-torch.sum(dvtm_with_lts_outputs)/len(docu))\n",
    "    dvtm_with_lts_theta = torch.sum(dvtm_with_lts_thetas, dim=0, keepdim=True)\n",
    "    dvtm_with_lts_theta = dvtm_with_lts_theta/torch.sum(dvtm_with_lts_theta)\n",
    "#     topk_val, topk_idxs = torch.topk(dvtm_with_lts_theta, k=3, dim=1)\n",
    "#     topk_idxs = torch.squeeze(topk_idxs)\n",
    "#     topk_idxs = topk_idxs.detach().cpu().numpy()\n",
    "#     topk_idxs_with_lts = set(topk_idxs.tolist())\n",
    "    \n",
    "#     if topk_idxs_without_lts.intersection(fixed_set) != fixed_set and topk_idxs_with_lts.intersection(fixed_set) != fixed_set:\n",
    "    dvtm_without_lts_theta_lst.append(dvtm_without_lts_theta.detach().cpu())\n",
    "    dvtm_with_lts_theta_lst.append(dvtm_with_lts_theta.detach().cpu())\n",
    "    ppl_data.append((myetm_ppl, dvtm_without_lts_ppl, dvtm_with_lts_ppl, dvtm_with_lts_ppl-myetm_ppl, dvtm_with_lts_ppl-dvtm_without_lts_ppl))\n",
    "#     break\n",
    "dvtm_without_lts_theta = torch.cat(dvtm_without_lts_theta_lst, dim=0)\n",
    "dvtm_without_lts_theta = dvtm_without_lts_theta.numpy()\n",
    "\n",
    "dvtm_with_lts_theta = torch.cat(dvtm_with_lts_theta_lst, dim=0)\n",
    "dvtm_with_lts_theta = dvtm_with_lts_theta.numpy()\n",
    "\n",
    "print(\"DVITM without lts Test Dataset Topic Proportion Shape: \"+ str(dvtm_without_lts_theta.shape))\n",
    "print(\"DVITM with lts Test Dataset Topic Proportion Shape: \"+ str(dvtm_with_lts_theta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hard, soft, multi-vqvae\n",
    "import math\n",
    "\n",
    "soft_theta_lst = []\n",
    "hard_theta_lst = []\n",
    "multi_theta_lst = []\n",
    "ppl_data = []\n",
    "\n",
    "for idx, docu in enumerate(tqdm(test_dataset, desc='Please wait..')):\n",
    "    docu = docu.cuda()\n",
    "    hard_outputs, hard_thetas, _ = hard_pretrained(docu, \"test\")\n",
    "    hard_ppl = math.exp(-torch.sum(hard_outputs)/len(docu))\n",
    "    hard_theta = torch.sum(hard_thetas, dim=0, keepdim=True)\n",
    "    hard_theta = hard_theta/torch.sum(hard_theta)\n",
    "    hard_theta_lst.append(hard_theta.detach().cpu())\n",
    "#     print(hard_theta.shape)\n",
    "#     print(hard_ppl)\n",
    "    \n",
    "    \n",
    "    soft_outputs, soft_theta,_ = soft_pretrained(docu, \"test\")\n",
    "    soft_ppl = math.exp(-torch.sum(soft_outputs)/len(docu))\n",
    "    soft_theta_lst.append(soft_theta.detach().cpu())\n",
    "#     print(soft_theta.shape)\n",
    "#     print(soft_ppl)\n",
    "    \n",
    "    multi_outputs, multi_thetas,_ = multi_pretrained(docu, \"test\")\n",
    "    multi_ppl = math.exp(-torch.sum(multi_outputs)/len(docu))\n",
    "    multi_theta = torch.mean(multi_thetas, dim=0)\n",
    "    multi_theta = torch.sum(multi_theta, dim=0, keepdim=True)\n",
    "    multi_theta = multi_theta/torch.sum(multi_theta)\n",
    "    multi_theta_lst.append(multi_theta.detach().cpu())\n",
    "#     print(multi_theta.shape)\n",
    "#     print(multi_ppl)\n",
    "#     break\n",
    "    \n",
    "    ppl_data.append((hard_ppl, soft_ppl, multi_ppl, multi_ppl-hard_ppl))\n",
    "\n",
    "hard_theta = torch.cat(hard_theta_lst, dim=0)\n",
    "hard_theta = hard_theta.numpy()\n",
    "\n",
    "soft_theta = torch.cat(soft_theta_lst, dim=0)\n",
    "soft_theta = soft_theta.numpy()\n",
    "\n",
    "multi_theta = torch.cat(multi_theta_lst, dim=0)\n",
    "multi_theta = multi_theta.numpy()\n",
    "\n",
    "print(\"Hard VQVAE Test Dataset Topic Proportion Shape: \"+ str(hard_theta.shape))\n",
    "print(\"Soft VQVAE Test Dataset Topic Proportion Shape: \"+ str(soft_theta.shape))\n",
    "print(\"Multi VQVAE Test Dataset Topic Proportion Shape: \"+ str(multi_theta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_df_data = np.array(ppl_data)\n",
    "df = pd.DataFrame(ppl_df_data, columns=['hard_ppl','soft_ppl','multi_ppl','diff_ppl_multi_hard'])\n",
    "df.sort_values(by=['diff_ppl_multi_hard']).tail(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_df_data = np.array(ppl_data)\n",
    "df = pd.DataFrame(ppl_df_data, columns=['etm','dvtm_without_lts','dvtm_with_lts','diff_ppl_dvtm_with_etm','diff_ppl_dvtm_with_dvtm_without'])\n",
    "pd.set_option('display.max_rows', df.shape[0]+1)\n",
    "# df.loc[3844]\n",
    "# df.sort_values(by=['diff_ppl_dvtm_with_etm','diff_ppl_dvtm_with_dvtm_without']).head(200)\n",
    "\n",
    "df.loc[(df['dvtm_without_lts'] < df['etm']) & (df['dvtm_with_lts'] < df['dvtm_without_lts'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df.loc[(df['dvtm_without_lts'] < df['etm']) & (df['dvtm_with_lts'] < df['dvtm_without_lts'])]\n",
    "df_subset_idxs = list(df_subset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#vqtms: final examples: 3907, 3844\n",
    "#dvitms: final examples: \n",
    "count = 1\n",
    "\n",
    "num_topics = 20\n",
    "doc_idxs = [3871]\n",
    "for doc_idx in doc_idxs:\n",
    "    fig = make_subplots(rows=5, \n",
    "                    cols=1,  \n",
    "                    subplot_titles=(\"CGS-LDA\",\"ProdLDA\", \"ETM\",'DVITM without lts loss','DVITM with lts loss'))\n",
    "#                    specs=[[{\"type\": \"table\"},{\"type\": \"table\"},{\"type\": \"table\"},{\"type\": \"table\"}],\n",
    "#                           [{\"type\": \"heatmap\"},{\"type\": \"heatmap\"},{\"type\": \"heatmap\"},{\"type\": \"heatmap\"}]])\n",
    "\n",
    "#     count = count + 1\n",
    "#     lda_entropy_trace = go.Heatmap(z = np.expand_dims(lda_theta[doc_idx],axis=0),\n",
    "#                                         x = np.arange(num_topics),\n",
    "#                                         y = np.array([(df_entropy['doc'][doc_idx],\" \", round(df_entropy['lda'][doc_idx],2))]),\n",
    "#                                         #y = np.array([round(df_entropy['lda'][doc_idx],2)]),\n",
    "#                                         showscale=False)\n",
    "#                                         #colorscale='gray')\n",
    "    \n",
    "#     myetm_entropy_trace = go.Heatmap(z = np.expand_dims(myetm_theta[doc_idx],axis=0),\n",
    "#                                         x = np.arange(num_topics),\n",
    "#                                         y = np.array([round(df_entropy['etm'][doc_idx],2)]),\n",
    "#                                         showscale=False,\n",
    "#                                         colorscale='gray')\n",
    "#     tokens_trace = go.Table(header=dict(values=[\"Tokens\"],font=dict(size=10),align=\"left\"),\n",
    "#                             cells=dict(values=[docu_tokens_lst[doc_idx]],align = \"left\"))\n",
    "    print(docu_tokens_lst[doc_idx])\n",
    "    lda_theta_trace = go.Heatmap(z = np.expand_dims(lda_theta[doc_idx],axis=0),\n",
    "                                        x = np.arange(num_topics),\n",
    "                                        y = [''],\n",
    "                                        #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "                                        showscale=False,\n",
    "                                        colorscale='algae')\n",
    "    \n",
    "    prodlda_theta_trace = go.Heatmap(z = np.expand_dims(myprodlda_theta[doc_idx],axis=0),\n",
    "                                        x = np.arange(num_topics),\n",
    "                                        y = [''],\n",
    "                                        #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "                                        showscale=False,\n",
    "                                        colorscale='algae')\n",
    "    \n",
    "    \n",
    "    etm_theta_trace = go.Heatmap(z = np.expand_dims(myetm_theta[doc_idx],axis=0),\n",
    "                                        x = np.arange(num_topics),\n",
    "                                        y = [''],\n",
    "                                        #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "                                        showscale=False,\n",
    "                                        colorscale='algae')\n",
    "    \n",
    "    dvtm_without_lts_theta_trace = go.Heatmap(z = np.expand_dims(dvtm_without_lts_theta[doc_idx],axis=0),\n",
    "                                    x = np.arange(num_topics),\n",
    "                                    y = [''],\n",
    "                                    #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "                                    showscale=False,\n",
    "                                    colorscale='algae')\n",
    "    \n",
    "    dvtm_with_lts_theta_trace = go.Heatmap(z = np.expand_dims(dvtm_with_lts_theta[doc_idx],axis=0),\n",
    "                                    x = np.arange(num_topics),\n",
    "                                    y = [''],\n",
    "                                    #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "                                    showscale=False,\n",
    "                                    colorscale='algae')\n",
    "    \n",
    "#     hard_theta_trace = go.Heatmap(z = np.expand_dims(hard_theta[doc_idx],axis=0),\n",
    "#                                         x = np.arange(num_topics),\n",
    "#                                         y = [''],\n",
    "#                                         #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "#                                         showscale=False,\n",
    "#                                         colorscale='algae')\n",
    "        \n",
    "#     soft_theta_trace = go.Heatmap(z = np.expand_dims(soft_theta[doc_idx],axis=0),\n",
    "#                                         x = np.arange(num_topics),\n",
    "#                                         y = [''],\n",
    "#                                         #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "#                                         showscale=False,\n",
    "#                                         colorscale='algae')\n",
    "#     multi_theta_trace = go.Heatmap(z = np.expand_dims(multi_theta[doc_idx],axis=0),\n",
    "#                                         x = np.arange(num_topics),\n",
    "#                                         y = [''],\n",
    "#                                         #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "#                                         showscale=False,\n",
    "#                                         colorscale='algae')\n",
    "#     fig.append_trace(tokens_trace , count,1)\n",
    "    fig.append_trace(lda_theta_trace , 1, count)\n",
    "    fig.append_trace(prodlda_theta_trace , 2, count)\n",
    "    fig.append_trace(etm_theta_trace , 3, count)\n",
    "    fig.append_trace(dvtm_without_lts_theta_trace , 4, count)\n",
    "    fig.append_trace(dvtm_with_lts_theta_trace , 5, count)\n",
    "        \n",
    "#     fig.append_trace(hard_theta_trace , 3, count)\n",
    "#     fig.append_trace(soft_theta_trace , 4, count)\n",
    "#     fig.append_trace(multi_theta_trace , 5, count)\n",
    "\n",
    "    fig.update_layout(height=500, width=500)\n",
    "    fig.update_xaxes(nticks=num_topics, type='category',showticklabels=True, tickfont=dict(size=8) )\n",
    "    fig.update_yaxes(showticklabels=True, type='category')\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "# 0\t0.46368\t[god, christian, jesus, people, bible, church, life, religion, faith, man]\n",
    "# 1\t0.32561\t[file, entry, program, line, output, section, set, return, error, read]\n",
    "# 2\t0.37114\t[university, fax, phone, write, internet, center, research, computer, article, information]\n",
    "# 3\t0.35226\t[time, back, bike, write, thing, make, dod, ride, turn, leave]\n",
    "# 4\t0.32547\t[power, ground, wire, work, water, current, circuit, make, sound, run]\n",
    "# 5\t0.32408\t[number, write, point, article, time, rate, bit, give, man, difference]\n",
    "# 6\t0.33109\t[problem, article, people, food, drug, write, study, year, time, effect]\n",
    "# 7\t0.37060\t[gun, law, people, state, government, weapon, crime, police, kill, case]\n",
    "# 8\t0.37749\t[drive, card, disk, system, problem, driver, mac, work, memory, dos]\n",
    "# 9\t0.44660\t[people, turkish, armenian, armenians, woman, kill, child, turkey, armenia, turks]\n",
    "# 10\t0.33743\t[question, make, argument, exist, claim, write, people, point, evidence, thing]\n",
    "# 11\t0.46893\t[israel, war, jews, israeli, state, write, article, people, country, world]\n",
    "# 12\t0.34694\t[president, make, work, money, people, tax, year, job, pay, clinton]\n",
    "# 13\t0.59197\t[game, team, play, year, player, win, season, hockey, score, league]\n",
    "# 14\t0.37982\t[key, chip, encryption, system, government, clipper, security, privacy, phone, secure]\n",
    "# 15\t0.38742\t[car, buy, price, sell, good, sale, offer, drive, pay, engine]\n",
    "# 16\t0.32969\t[post, send, list, book, group, information, mail, address, include, copy]\n",
    "# 17\t0.41678\t[window, file, image, program, run, display, version, application, server, software]\n",
    "# 18\t0.33229\t[write, article, people, thing, make, hear, read, good, post, opinion]\n",
    "# 19\t0.45590\t[space, launch, nasa, system, earth, satellite, year, orbit, project, mission]\n",
    "#ProdLDA\n",
    "# 0\t0.4658\t[minnesota, angeles, toronto, san, montreal, vancouver, ottawa, stanley, calgary, louis]\n",
    "# 1\t0.3945\t[finish, play, team, hitter, offense, smith, defensive, ice, tie, hit]\n",
    "# 2\t0.5129\t[christian, god, scripture, interpretation, jesus, resurrection, teaching, doctrine, existence, holy]\n",
    "# 3\t0.3626\t[troops, israel, border, turks, army, israeli, fire, arab, civilian, minority]\n",
    "# 4\t0.5116\t[ram, windows, quadra, fine, speed, scsi, faster, microsoft, apple, external]\n",
    "# 5\t0.3740\t[entry, char, compile, db, file, section, variable, contest, distribution, remark]\n",
    "# 6\t0.5557\t[christian, scripture, god, teaching, resurrection, existence, jesus, christianity, doctrine, biblical]\n",
    "# 7\t0.3890\t[ibm, interface, transfer, external, virtual, cpu, ram, dec, path, default]\n",
    "# 8\t0.4294\t[fire, car, safety, btw, andy, surrender, country, rider, cold, stupid]\n",
    "# 9\t0.4889\t[wiretap, escrow, drug, agency, warrant, clipper, illegal, encryption, crime, country]\n",
    "# 10\t0.4657\t[wiretap, encryption, escrow, nsa, chip, clipper, pat, cheaper, scheme, car]\n",
    "# 11\t0.4874\t[turks, armenian, army, turkish, russian, armenia, united, mountain, armenians, director]\n",
    "# 12\t0.4587\t[music, fine, crash, ram, sl, external, simm, honda, wm, hd]\n",
    "# 13\t0.4824\t[bmw, car, bike, rider, baseball, btw, cop, ball, hit, motorcycle]\n",
    "# 14\t0.4628\t[satellite, mission, distribute, nasa, spacecraft, km, space, earth, distribution, module]\n",
    "# 15\t0.5309\t[fine, ram, windows, apple, amp, simm, external, scsi, crash, quadra]\n",
    "# 16\t0.4720\t[agency, encryption, cryptography, telephone, wiretap, enforcement, des, privacy, distribution, encrypt]\n",
    "# 17\t0.3659\t[neighbor, christian, heart, harm, building, scripture, jesus, daughter, woman, holy]\n",
    "# 18\t0.4507\t[scsi, ram, hd, external, mhz, meg, scsus, ide, fine, bus]\n",
    "# 19\t0.5172\t[sl, wm, ram, mi, external, hd, connector, mg, mb, mw]\n",
    "#ETM\n",
    "# 0\t0.34885\t[get, know, one, say, think, like, see, thing, people, time]\n",
    "# 1\t0.3254\t[use, may, make, case, many, also, part, however, system, president]\n",
    "# 2\t0.45049\t[god, jesus, christian, say, believe, bible, one, christ, make, belief]\n",
    "# 3\t0.3682\t[gun, people, child, kill, drug, crime, weapon, police, case, claim]\n",
    "# 4\t0.52558\t[datum, space, db, launch, output, hus, widget, dod, nasa, sun]\n",
    "# 5\t0.33899\t[file, use, program, send, available, list, code, email, please, line]\n",
    "# 6\t0.41394\t[hockey, team, new, division, san, canada, nhl, toronto, york, gm]\n",
    "# 7\t0.34391\t[new, look, buy, price, good, sell, include, package, offer, like]\n",
    "# 8\t0.38514\t[car, power, use, drive, speed, engine, wire, water, fast, low]\n",
    "# 9\t0.46506\t[game, year, play, win, team, player, season, go, good, run]\n",
    "# 10\t0.33123\t[information, make, get, please, mail, use, go, file, help, take]\n",
    "# 11\t0.3378\t[book, first, one, time, internet, study, earth, author, history, find]\n",
    "# 12\t0.35159\t[university, group, science, information, computer, fax, center, year, call, research]\n",
    "# 13\t0.36281\t[thanks, david, john, appreciate, steve, mark, wonder, jim, mike, michael]\n",
    "# 14\t0.37497\t[write, article, post, question, read, opinion, ask, please, yes, answer]\n",
    "# 15\t0.33965\t[period, la, pt, vs, de, van, pp, cal, power, second]\n",
    "# 16\t0.36068\t[key, government, law, use, encryption, state, chip, public, right, security]\n",
    "# 17\t0.33544\t[go, take, back, one, day, put, get, right, also, call]\n",
    "# 18\t0.39872\t[use, drive, system, window, card, run, windows, disk, problem, image]\n",
    "# 19\t0.48538\t[israel, people, war, israeli, jews, turkish, armenians, country, armenian, government]\n",
    "\n",
    "#DVITM without lts loss\n",
    "# 0\t0.49933\t[msg, eat, heaven, koresh, love, one, food, jesus, hell, sin]\n",
    "# 1\t0.49305\t[koresh, love, heaven, eat, msg, one, hell, sin, pray, cult]\n",
    "# 2\t0.45914\t[msg, eat, koresh, food, heaven, compound, one, love, hell, heart]\n",
    "# 3\t0.45914\t[msg, eat, koresh, food, heaven, compound, love, one, hell, heart]\n",
    "# 4\t0.46723\t[msg, eat, koresh, love, heaven, one, compound, hell, food, pray]\n",
    "# 5\t0.44593\t[msg, eat, koresh, food, compound, heaven, one, heart, fbi, love]\n",
    "# 6\t0.46723\t[msg, eat, koresh, food, heaven, one, love, pray, hell, compound]\n",
    "# 7\t0.46723\t[msg, eat, koresh, heaven, love, one, hell, pray, food, compound]\n",
    "# 8\t0.42821\t[msg, eat, koresh, food, compound, heaven, one, heart, love, child]\n",
    "# 9\t0.45914\t[msg, eat, koresh, food, one, heaven, compound, love, hell, heart]\n",
    "# 10\t0.42445\t[mother, father, child, msg, love, eat, die, compound, koresh, heart]\n",
    "# 11\t0.45914\t[msg, eat, koresh, food, heaven, one, love, compound, hell, heart]\n",
    "# 12\t0.49725\t[love, koresh, eat, pray, heaven, one, hell, msg, sin, god]\n",
    "# 13\t0.47096\t[card, rg, mr, ax, vs, eus, oo, dos, bhj, slot]\n",
    "# 14\t0.50001\t[lebanese, resistance, israeli, buffer, zone, lebanon, israel, palestinian, civilian, bomb]\n",
    "# 15\t0.46723\t[msg, eat, koresh, one, food, heaven, love, hell, pray, compound]\n",
    "# 16\t0.45914\t[msg, eat, koresh, compound, food, one, heaven, love, heart, hell]\n",
    "# 17\t0.45914\t[msg, eat, koresh, love, one, heaven, hell, compound, food, heart]\n",
    "# 18\t0.46723\t[msg, eat, koresh, heaven, one, love, food, compound, pray, hell]\n",
    "# 19\t0.42821\t[msg, koresh, eat, compound, one, food, heart, heaven, love, child]\n",
    "\n",
    "#DVITM with lts loss\n",
    "# 0\t0.56466\t[andor, btw, heat, flame, reach, hd, eliminate, tax, shipping, hus]\n",
    "# 1\t0.36240\t[sexual, provide, sex, male, physical, animal, morality, natural, activity, normal]\n",
    "# 2\t0.36994\t[need, form, hus, surface, side, change, ring, normal, compound, order]\n",
    "# 3\t0.55837\t[compound, xterm, andor, plane, initial, patient, rg, colormap, batf, hus]\n",
    "# 4\t0.31337\t[necessarily, accident, least, person, cover, society, fact, moral, understanding, eternal]\n",
    "# 5\t0.33684\t[show, april, russia, team, tv, local, allen, movie, board, satellite]\n",
    "# 6\t0.33354\t[must, responsibility, human, conflict, means, desire, without, action, accept, upon]\n",
    "# 7\t0.49377\t[compound, batf, flame, fire, btw, dod, proper, andor, plane, scsus]\n",
    "# 8\t0.32176\t[compound, strip, wide, main, ground, rule, inside, four, shop, path]\n",
    "# 9\t0.38172\t[andor, part, nuclear, call, server, include, char, date, game, concept]\n",
    "# 10\t0.38304\t[technology, company, network, phone, business, corporation, development, agency, sector, government]\n",
    "# 11\t0.50270\t[dod, target, zone, al, civilian, oo, batf, rg, mi, border]\n",
    "# 12\t0.44349\t[race, impact, compare, hitter, previous, pitcher, truck, mark, braves, far]\n",
    "# 13\t0.43725\t[hockey, canada, state, cover, southern, board, engineering, soon, canadian, helmet]\n",
    "# 14\t0.54549\t[colormap, compound, initial, andor, xlib, range, distribution, detail, info, xterm]\n",
    "# 15\t0.51924\t[koresh, batf, believe, die, btw, compound, else, hell, eternal, everyone]\n",
    "# 16\t0.57708\t[xterm, hus, colormap, dod, pl, buf, char, printf, int, routine]\n",
    "# 17\t0.41406\t[like, much, probably, look, feeling, worse, feel, hear, mine, sound]\n",
    "# 18\t0.48340\t[every, compound, hit, writer, dod, imho, btw, eat, injury, interpretation]\n",
    "# 19\t0.40255\t[cell, range, colormap, sequence, distance, input, null, match, compound, clock]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "# 0\t0.46368\t[god, christian, jesus, people, bible, church, life, religion, faith, man]\n",
    "# 1\t0.32561\t[file, entry, program, line, output, section, set, return, error, read]\n",
    "# 2\t0.37114\t[university, fax, phone, write, internet, center, research, computer, article, information]\n",
    "# 3\t0.35226\t[time, back, bike, write, thing, make, dod, ride, turn, leave]\n",
    "# 4\t0.32547\t[power, ground, wire, work, water, current, circuit, make, sound, run]\n",
    "# 5\t0.32408\t[number, write, point, article, time, rate, bit, give, man, difference]\n",
    "# 6\t0.33109\t[problem, article, people, food, drug, write, study, year, time, effect]\n",
    "# 7\t0.37060\t[gun, law, people, state, government, weapon, crime, police, kill, case]\n",
    "# 8\t0.37749\t[drive, card, disk, system, problem, driver, mac, work, memory, dos]\n",
    "# 9\t0.44660\t[people, turkish, armenian, armenians, woman, kill, child, turkey, armenia, turks]\n",
    "# 10\t0.33743\t[question, make, argument, exist, claim, write, people, point, evidence, thing]\n",
    "# 11\t0.46893\t[israel, war, jews, israeli, state, write, article, people, country, world]\n",
    "# 12\t0.34694\t[president, make, work, money, people, tax, year, job, pay, clinton]\n",
    "# 13\t0.59197\t[game, team, play, year, player, win, season, hockey, score, league]\n",
    "# 14\t0.37982\t[key, chip, encryption, system, government, clipper, security, privacy, phone, secure]\n",
    "# 15\t0.38742\t[car, buy, price, sell, good, sale, offer, drive, pay, engine]\n",
    "# 16\t0.32969\t[post, send, list, book, group, information, mail, address, include, copy]\n",
    "# 17\t0.41678\t[window, file, image, program, run, display, version, application, server, software]\n",
    "# 18\t0.33229\t[write, article, people, thing, make, hear, read, good, post, opinion]\n",
    "# 19\t0.45590\t[space, launch, nasa, system, earth, satellite, year, orbit, project, mission]\n",
    "#ETM\n",
    "# 0\t0.34885\t[get, know, one, say, think, like, see, thing, people, time]\n",
    "# 1\t0.3254\t[use, may, make, case, many, also, part, however, system, president]\n",
    "# 2\t0.45049\t[god, jesus, christian, say, believe, bible, one, christ, make, belief]\n",
    "# 3\t0.3682\t[gun, people, child, kill, drug, crime, weapon, police, case, claim]\n",
    "# 4\t0.52558\t[datum, space, db, launch, output, hus, widget, dod, nasa, sun]\n",
    "# 5\t0.33899\t[file, use, program, send, available, list, code, email, please, line]\n",
    "# 6\t0.41394\t[hockey, team, new, division, san, canada, nhl, toronto, york, gm]\n",
    "# 7\t0.34391\t[new, look, buy, price, good, sell, include, package, offer, like]\n",
    "# 8\t0.38514\t[car, power, use, drive, speed, engine, wire, water, fast, low]\n",
    "# 9\t0.46506\t[game, year, play, win, team, player, season, go, good, run]\n",
    "# 10\t0.33123\t[information, make, get, please, mail, use, go, file, help, take]\n",
    "# 11\t0.3378\t[book, first, one, time, science, study, earth, author, history, find]\n",
    "# 12\t0.35159\t[university, group, internet, information, computer, fax, center, year, call, research]\n",
    "# 13\t0.36281\t[thanks, david, john, appreciate, steve, mark, wonder, jim, mike, michael]\n",
    "# 14\t0.37497\t[write, article, post, question, read, opinion, ask, please, yes, answer]\n",
    "# 15\t0.33965\t[period, la, pt, vs, de, van, pp, cal, power, second]\n",
    "# 16\t0.36068\t[key, government, law, use, encryption, state, chip, public, right, security]\n",
    "# 17\t0.33544\t[go, take, back, one, day, put, get, right, also, call]\n",
    "# 18\t0.39872\t[use, drive, system, window, card, run, windows, disk, problem, image]\n",
    "# 19\t0.48538\t[israel, people, war, israeli, jews, turkish, armenians, country, armenian, government]\n",
    "#Hard\n",
    "# 0\t0.3978\t[murder, law, criminal, child, death, court, police, case, woman, man]\n",
    "# 1\t0.3923\t[article, page, journal, magazine, constitution, newspaper, editor, report, book, news]\n",
    "# 2\t0.3738\t[really, think, know, want, something, get, thing, feel, lot, anything]\n",
    "# 3\t0.3290\t[ball, back, right, water, inside, away, around, space, front, small]\n",
    "# 4\t0.6077\t[god, christ, jesus, christianity, christian, religious, religion, holy, church, faith]\n",
    "# 5\t0.3495\t[help, want, make, need, try, take, get, must, able, give]\n",
    "# 6\t0.4231\t[john, mike, david, steve, michael, james, chris, tom, jim, scott]\n",
    "# 7\t0.4001\t[write, read, publish, book, writer, please, reader, mail, page, author]\n",
    "# 8\t0.6378\t[software, pc, computer, server, disk, macintosh, windows, processor, interface, modem]\n",
    "# 9\t0.5482\t[car, vehicle, engine, driver, truck, bus, wheel, motor, drive, speed]\n",
    "# 10\t0.6024\t[ftp, unix, usenet, mb, newsgroup, pgp, server, toolkit, modem, faq]\n",
    "# 11\t0.3340\t[think, might, need, much, whether, want, even, something, really, anything]\n",
    "# 12\t0.3808\t[please, ask, call, tell, hear, want, answer, listen, know, let]\n",
    "# 13\t0.4515\t[use, method, technique, allow, equipment, manufacture, systems, tool, available, common]\n",
    "# 14\t0.6510\t[game, league, season, playoff, team, player, win, play, cup, hockey]\n",
    "# 15\t0.4908\t[mail, internet, phone, email, fax, information, telephone, service, access, services]\n",
    "# 16\t0.3348\t[people, many, population, number, least, among, million, dead, muslims, living]\n",
    "# 17\t0.4787\t[system, digital, software, systems, data, computer, use, video, standard, format]\n",
    "# 18\t0.4883\t[university, institute, science, research, college, education, program, professor, school, engineering]\n",
    "# 19\t0.3281\t[year, last, two, government, world, three, time, since, first, week]\n",
    "\n",
    "#Soft\n",
    "# 0\t0.40969\t[blood, hus, knife, cancer, die, death, woman, disease, drug, girl]\n",
    "# 1\t0.50091\t[university, science, research, institute, college, professor, engineering, education, physics, school]\n",
    "# 2\t0.35567\t[really, know, think, somebody, want, get, everybody, ca, thing, lot]\n",
    "# 3\t0.41657\t[near, station, line, area, north, road, east, west, across, south]\n",
    "# 4\t0.60767\t[god, christ, jesus, religion, christianity, religious, christian, holy, faith, church]\n",
    "# 5\t0.34947\t[help, want, get, make, try, take, need, must, able, give]\n",
    "# 6\t0.34497\t[think, know, anyone, anything, whether, believe, really, understand, something, anybody]\n",
    "# 7\t0.38435\t[write, read, publish, book, reader, writer, please, mail, tell, reading]\n",
    "# 8\t0.50053\t[billion, percent, price, million, rate, dollar, increase, higher, per, value]\n",
    "# 9\t0.59022\t[car, engine, driver, vehicle, truck, motorcycle, motor, wheel, bmw, honda]\n",
    "# 10\t0.63715\t[software, server, interface, pc, computer, windows, macintosh, disk, modem, user]\n",
    "# 11\t0.45493\t[use, system, systems, method, specific, function, common, data, type, standard]\n",
    "# 12\t0.42623\t[mail, please, fax, phone, email, call, telephone, message, address, information]\n",
    "# 13\t0.4241\t[two, three, one, four, five, six, second, another, single, first]\n",
    "# 14\t0.42306\t[john, mike, steve, david, michael, chris, james, tom, jim, scott]\n",
    "# 15\t0.41854\t[government, security, administration, military, state, federal, agency, policy, foreign, economic]\n",
    "# 16\t0.42642\t[people, many, say, among, muslims, americans, jews, population, number, live]\n",
    "# 17\t0.42113\t[article, magazine, journal, page, constitution, newspaper, editor, edition, book, publish]\n",
    "# 18\t0.62005\t[game, season, league, win, cup, match, playoff, team, final, play]\n",
    "# 19\t0.3505\t[really, good, pretty, little, something, lot, feel, bit, think, look]\n",
    "\n",
    "#Multi\n",
    "# 0\t0.5670\t[mb, homosexual, atheist, mg, cache, simm, valid, mhz, hitler, device]\n",
    "# 1\t0.3279\t[kevin, mike, justice, al, goal, director, brother, bush, think, son, leader]\n",
    "# 2\t0.3805\t[frequency, mhz, budget, space, digital, research, technology, useful, operate, domain]\n",
    "# 3\t0.4839\t[fax, mail, please, email, reply, phone, comment, sorry, posting, mailing]\n",
    "# 4\t0.3460\t[index, atlanta, rate, department, percent, fax, billion, economic, chicago, report]\n",
    "# 5\t0.5937\t[oo, eus, batf, simm, koresh, wm, stanley, mit, dod, nec]\n",
    "# 6\t0.4527\t[constitution, lewis, lebanese, murder, gordon, insurance, british, waco, wm, koresh]\n",
    "# 7\t0.3606\t[army, islamic, military, la, police, fire, heavy, palestinian, wing, tank]\n",
    "# 8\t0.3449\t[request, letter, food, secretary, case, congress, disease, director, judge, animal]\n",
    "# 9\t0.5344\t[scsi, rocket, spacecraft, hd, fuel, radar, ac, converter, station, motherboard]\n",
    "# 10\t0.6278\t[datum, buf, stephanopoulos, compression, pixel, vga, delete, bias, genocide, encrypt]\n",
    "# 11\t0.4720\t[datum, mw, turkey, islamic, firearm, armenians, turks, turkish, island, tank]\n",
    "# 12\t0.3695\t[goal, best, run, win, try, game, ahead, good, award, drive]\n",
    "# 13\t0.5676\t[algorithm, binary, unix, server, interface, byte, configuration, os, cpu, encryption]\n",
    "# 14\t0.5825\t[usenet, uucp, printf, btw, cryptography, newsgroup, vga, xt, widget, xlib]\n",
    "# 15\t0.3411\t[relative, economic, former, significant, israel, family, male, lebanon, slave, war]\n",
    "# 16\t0.4289\t[audio, bill, coverage, digital, law, card, services, full, cover, deep]\n",
    "# 17\t0.3409\t[state, religious, town, local, police, texas, political, power, culture, government]\n",
    "# 18\t0.4240\t[sick, care, option, son, older, insurance, family, child, choose, mine]\n",
    "# 19\t0.5686\t[calgary, oo, min, bay, beat, buffer, jumper, neighbor, toolkit, mi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docus_len = []\n",
    "# for docu in test_dataset:\n",
    "#     docus_len.append(len(docu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# test_files = sorted(glob.glob('../20ng/data/test_mallet/*.txt'))\n",
    "# len_file = []\n",
    "# for each in test_files:\n",
    "#     with open(each, 'r') as each_file:\n",
    "#         len_file.append((int(each.split('/')[4].replace('.txt','')), len(each_file.read().split(' '))))\n",
    "\n",
    "# df_file = pd.DataFrame(len_file, columns=['doc','len'])\n",
    "# df_file = df_file.sort_values(by='doc')\n",
    "# df_file = df_file.reset_index(drop=True)\n",
    "# print(df_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 9)\n",
    "#pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "entropy_data = []\n",
    "docu_count = 0\n",
    "\n",
    "for theta0, theta1, theta2, docu_len, docu in zip(lda_theta, myetm_theta, soft_theta, docus_len, test_dataset):\n",
    "    docu = [swapped_vocab_docu[idx] for idx in docu.numpy()]\n",
    "    docu = ' '.join(docu)\n",
    "    \n",
    "    lda_entropy = -np.sum(theta0 * np.log2(theta0))\n",
    "    etm_entropy = -np.sum(theta1 * np.log2(theta1))\n",
    "    soft_entropy = -np.sum(theta2 * np.log2(theta2))\n",
    "    \n",
    "#     if soft_negentropy > etm_negentropy and etm_negentropy > lda_negentropy:\n",
    "#         condn = True\n",
    "#     else:\n",
    "#         condn = False\n",
    "    diff_soft_etm = soft_entropy - etm_entropy\n",
    "    diff_etm_lda = etm_entropy - lda_entropy\n",
    "    diff_soft_lda = soft_entropy - lda_entropy\n",
    "    \n",
    "    #negentropy_data.append((docu_count, lda_negentropy, etm_negentropy, soft_negentropy, condn))\n",
    "    entropy_data.append((docu_count,\n",
    "                         docu_len,\n",
    "                         docu,\n",
    "                        lda_entropy, \n",
    "                        etm_entropy, \n",
    "                        soft_entropy,\n",
    "                        diff_etm_lda,\n",
    "                        diff_soft_etm,\n",
    "                        diff_soft_lda))\n",
    "\n",
    "    docu_count = docu_count + 1\n",
    "\n",
    "#df_entropy = pd.DataFrame(negentropy_data, columns=['doc_idx','lda','etm','soft','condn'])\n",
    "df_entropy = pd.DataFrame(entropy_data, columns=['doc_idx',\n",
    "                                                 'doc_len',\n",
    "                                                 'doc',\n",
    "                                                    'lda',\n",
    "                                                    'etm',\n",
    "                                                    'soft',\n",
    "                                                    'diff_etm_lda',\n",
    "                                                    'diff_soft_etm',\n",
    "                                                    'diff_soft_lda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_winning_docs_soft_lda = sum(n < 0 for n in df_entropy['diff_soft_lda'])\n",
    "stat_losing_docs_soft_lda = sum(n > 0 for n in df_entropy['diff_soft_lda'])\n",
    "print(stat_winning_docs_soft_lda)\n",
    "print(stat_losing_docs_soft_lda)\n",
    "\n",
    "stat_winning_docs_soft_etm = sum(n < 0 for n in df_entropy['diff_soft_etm'])\n",
    "stat_losing_docs_soft_etm = sum(n > 0 for n in df_entropy['diff_soft_etm'])\n",
    "print(stat_winning_docs_soft_etm)\n",
    "print(stat_losing_docs_soft_etm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docs where Soft is winning only against  ETM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entropy[(df_entropy['diff_soft_etm'] < 0)].sort_values(by='diff_soft_etm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docs where Soft is winning only against LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entropy[(df_entropy['diff_soft_lda'] < 0) & (df_entropy['doc_len']> 5)].sort_values(by='diff_soft_lda')[60:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#winning docs = 4805, 3616, 318, 2306, 62\n",
    "fig_entropy = make_subplots(rows=10, cols=2,  \n",
    "                    subplot_titles=(\"LDA\",\"Soft VQ-VAE\"))\n",
    "doc_idxs = [4805, 3616, 318, 2306, 62]\n",
    "count = 0 \n",
    "for doc_idx in doc_idxs:\n",
    "    count = count + 1\n",
    "    lda_entropy_trace = go.Heatmap(z = np.expand_dims(lda_theta[doc_idx],axis=0),\n",
    "                                        x = np.arange(num_topics),\n",
    "                                        y = np.array([(df_entropy['doc'][doc_idx],\" \", round(df_entropy['lda'][doc_idx],2))]),\n",
    "                                        #y = np.array([round(df_entropy['lda'][doc_idx],2)]),\n",
    "                                        showscale=False)\n",
    "                                        #colorscale='gray')\n",
    "    \n",
    "#     myetm_entropy_trace = go.Heatmap(z = np.expand_dims(myetm_theta[doc_idx],axis=0),\n",
    "#                                         x = np.arange(num_topics),\n",
    "#                                         y = np.array([round(df_entropy['etm'][doc_idx],2)]),\n",
    "#                                         showscale=False,\n",
    "#                                         colorscale='gray')\n",
    "    \n",
    "    soft_entropy_trace = go.Heatmap(z = np.expand_dims(soft_theta[doc_idx],axis=0),\n",
    "                                        x = np.arange(num_topics),\n",
    "                                        y = np.array([round(df_entropy['soft'][doc_idx],2)]),\n",
    "                                        #y = np.array([(df_entropy['doc'][doc_idx],\"  \"+str(round(df_entropy['soft'][doc_idx],2)))]),\n",
    "                                        showscale=False)\n",
    "                                        #colorscale='gray')\n",
    "    \n",
    "    fig_entropy.append_trace(lda_entropy_trace , count,1)\n",
    "    #fig_entropy.append_trace(myetm_entropy_trace , count,2)\n",
    "    fig_entropy.append_trace(soft_entropy_trace , count,2)\n",
    "\n",
    "fig_entropy.update_layout(height=500)\n",
    "fig_entropy.update_xaxes(nticks=num_topics, type='category',showticklabels=True, tickfont=dict(size=8) )\n",
    "fig_entropy.update_yaxes(showticklabels=True, type='category')\n",
    "\n",
    "\n",
    "fig_entropy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
